{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import nbformat\n",
    "from nbformat.v4 import new_code_cell, new_markdown_cell, new_raw_cell\n",
    "from nbconvert import HTMLExporter\n",
    "from nbconvert.preprocessors import ExecutePreprocessor, CellExecutionError\n",
    "import re\n",
    "import textwrap\n",
    "import shutil\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../accy575-sp-2023/02-pcard-submissions\\\\anacondas_324892_7729312_PCard-after-milestone-1.ipynb',\n",
       " '../../accy575-sp-2023/02-pcard-submissions\\\\ancillaries_334459_7718488_PCard_Ancillaries.ipynb',\n",
       " '../../accy575-sp-2023/02-pcard-submissions\\\\df_324476_7727330_PCard-after-milestone.ipynb',\n",
       " '../../accy575-sp-2023/02-pcard-submissions\\\\excellent_65151_7725650_PCard_Excellent.ipynb',\n",
       " '../../accy575-sp-2023/02-pcard-submissions\\\\get_an_a_323576_7703657_PCard-get_an_A.ipynb']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpaths = glob.glob('../../accy575-sp-2023/02-pcard-submissions/*.ipynb')\n",
    "fpaths = list(filter(lambda f: not f.endswith('-graded.ipynb'), fpaths))\n",
    "fpaths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../submissions/exercise-05\\\\chenchloe_83497_7409117_exercise_05_pandas_filtering_sorting.ipynb',\n",
       " '../submissions/exercise-05\\\\esparzajoel_51499_7451315_Copy_of_exercise_05_pandas_filtering_sorting.ipynb',\n",
       " '../submissions/exercise-05\\\\tabornatalie_8175_7404582_exercise_05_pandas_filtering_sorting.ipynb']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpaths = glob.glob('../submissions/exercise-05/*.ipynb')\n",
    "fpaths = list(filter(lambda f: not f.endswith('-graded.ipynb'), fpaths))\n",
    "fpaths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================\n",
      "Running ../../accy575-sp-2023/02-pcard-submissions\\anacondas_324892_7729312_PCard-after-milestone-1.ipynb successful\n",
      "Stored graded result as JSON to ../../accy575-sp-2023/02-pcard-submissions\\anacondas_324892_7729312_PCard-after-milestone-1-result.json\n",
      "{'filename': 'anacondas_324892_7729312_PCard-after-milestone-1-graded.ipynb', 'grading_finished_at': '2023-02-18T05:22:07.940681', 'grading_duration_in_seconds': 21.16, 'learner_score': 69, 'total_available': 82, 'num_test_cases': 20, 'num_passed_cases': 18, 'num_failed_cases': 2, 'summary': 'File: anacondas_324892_7729312_PCard-after-milestone-1-graded.ipynb\\nScore: 69 out of 82\\nPassed 18 out of 20 test cases\\nGrading took 21.16 seconds\\n\\nTest Case Summary\\n-----------------\\n1B: Create a list of column names passed: 2 out of 2 points\\n-----------------\\n1C: Read the CSV files into Pandas DataFrames passed: 3 out of 3 points\\n-----------------\\n1D: Filter only relevant data passed: 2 out of 2 points\\n-----------------\\n1E: Concatenate all transactions passed: 2 out of 2 points\\n-----------------\\n1F: Convert different date string formats into datetime types passed: 4 out of 4 points\\n-----------------\\n1G: Convert transaction_date and posted_date columns to datetime types passed: 3 out of 3 points\\n-----------------\\n1H: Convert amount column to float type passed: 3 out of 3 points\\n-----------------\\n1I: Clean up text (string) columns passed: 3 out of 3 points\\n-----------------\\n2A: Filter 2014 transactions passed: 2 out of 2 points\\n-----------------\\n2B: Create a concatenated unique_name column passed: 2 out of 2 points\\n-----------------\\n2C: Create a month column passed: 2 out of 2 points\\n-----------------\\n2D: Remove unused columns passed: 2 out of 2 points\\n-----------------\\nAnalysis 1: Most expensive transactions by month in 2014 passed: 8 out of 8 points\\n-----------------\\nAnalysis 2: Cardholders with the largest number of transactions in 2014 passed: 6 out of 6 points\\n-----------------\\nAnalysis 3: Total amount spent by Merchant Category Code passed: 6 out of 6 points\\n-----------------\\nICT 1: Employee monthly spending limit passed: 6 out of 6 points\\n-----------------\\nICT 2: Splitting a large purchase into multiple smaller transactions passed: 7 out of 7 points\\n-----------------\\nFT 1: Benford Analysis failed: 0 out of 7 points\\n[Autograder Output]\\nAssertionError: DataFrame.columns are different\\n\\nDataFrame.columns values are different (33.33333 %)\\n[left]:  Index([\\'first_digit\\', \\'count\\', \\'percentage\\'], dtype=\\'object\\')\\n[right]: Index([\\'first_digit\\', \\'count\\', \\'percent\\'], dtype=\\'object\\')\\n\\n-----------------\\nFT 2A: Duplicate Transactions passed: 6 out of 6 points\\n-----------------\\nFT 2B: Duplicate Transactions (Worst Offenders) failed: 0 out of 6 points\\n[Autograder Output]\\nAssertionError: DataFrame.iloc[:, 0] (column name=\"unique_name\") are different\\n\\nDataFrame.iloc[:, 0] (column name=\"unique_name\") values are different (100.0 %)\\n[index]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\\n[left]:  [gordon, d, haseley, s, roaseau, k, bishop, k, clark, k, longan, s, roaseau, k, sanderson, d, longan, s, robinson, d]\\n[right]: [bailey, j, gebhart, g, knott, s, hines, g, sisney, d, tornakian, m, robinson, s, davis, e, roaseau, k, bryans, m]\\n\\n'}\n",
      "=============================\n",
      "Running ../../accy575-sp-2023/02-pcard-submissions\\ancillaries_334459_7718488_PCard_Ancillaries.ipynb successful\n",
      "Stored graded result as JSON to ../../accy575-sp-2023/02-pcard-submissions\\ancillaries_334459_7718488_PCard_Ancillaries-result.json\n",
      "{'filename': 'ancillaries_334459_7718488_PCard_Ancillaries-graded.ipynb', 'grading_finished_at': '2023-02-18T05:22:32.549698', 'grading_duration_in_seconds': 21.33, 'learner_score': 82, 'total_available': 82, 'num_test_cases': 20, 'num_passed_cases': 20, 'num_failed_cases': 0, 'summary': 'File: ancillaries_334459_7718488_PCard_Ancillaries-graded.ipynb\\nScore: 82 out of 82\\nPassed 20 out of 20 test cases\\nGrading took 21.33 seconds\\n\\nTest Case Summary\\n-----------------\\n1B: Create a list of column names passed: 2 out of 2 points\\n-----------------\\n1C: Read the CSV files into Pandas DataFrames passed: 3 out of 3 points\\n-----------------\\n1D: Filter only relevant data passed: 2 out of 2 points\\n-----------------\\n1E: Concatenate all transactions passed: 2 out of 2 points\\n-----------------\\n1F: Convert different date string formats into datetime types passed: 4 out of 4 points\\n-----------------\\n1G: Convert transaction_date and posted_date columns to datetime types passed: 3 out of 3 points\\n-----------------\\n1H: Convert amount column to float type passed: 3 out of 3 points\\n-----------------\\n1I: Clean up text (string) columns passed: 3 out of 3 points\\n-----------------\\n2A: Filter 2014 transactions passed: 2 out of 2 points\\n-----------------\\n2B: Create a concatenated unique_name column passed: 2 out of 2 points\\n-----------------\\n2C: Create a month column passed: 2 out of 2 points\\n-----------------\\n2D: Remove unused columns passed: 2 out of 2 points\\n-----------------\\nAnalysis 1: Most expensive transactions by month in 2014 passed: 8 out of 8 points\\n-----------------\\nAnalysis 2: Cardholders with the largest number of transactions in 2014 passed: 6 out of 6 points\\n-----------------\\nAnalysis 3: Total amount spent by Merchant Category Code passed: 6 out of 6 points\\n-----------------\\nICT 1: Employee monthly spending limit passed: 6 out of 6 points\\n-----------------\\nICT 2: Splitting a large purchase into multiple smaller transactions passed: 7 out of 7 points\\n-----------------\\nFT 1: Benford Analysis passed: 7 out of 7 points\\n-----------------\\nFT 2A: Duplicate Transactions passed: 6 out of 6 points\\n-----------------\\nFT 2B: Duplicate Transactions (Worst Offenders) passed: 6 out of 6 points\\n'}\n",
      "=============================\n",
      "Running ../../accy575-sp-2023/02-pcard-submissions\\df_324476_7727330_PCard-after-milestone.ipynb successful\n",
      "Stored graded result as JSON to ../../accy575-sp-2023/02-pcard-submissions\\df_324476_7727330_PCard-after-milestone-result.json\n",
      "{'filename': 'df_324476_7727330_PCard-after-milestone-graded.ipynb', 'grading_finished_at': '2023-02-18T05:22:57.940717', 'grading_duration_in_seconds': 22.13, 'learner_score': 76, 'total_available': 82, 'num_test_cases': 20, 'num_passed_cases': 19, 'num_failed_cases': 1, 'summary': 'File: df_324476_7727330_PCard-after-milestone-graded.ipynb\\nScore: 76 out of 82\\nPassed 19 out of 20 test cases\\nGrading took 22.13 seconds\\n\\nTest Case Summary\\n-----------------\\n1B: Create a list of column names passed: 2 out of 2 points\\n-----------------\\n1C: Read the CSV files into Pandas DataFrames passed: 3 out of 3 points\\n-----------------\\n1D: Filter only relevant data passed: 2 out of 2 points\\n-----------------\\n1E: Concatenate all transactions passed: 2 out of 2 points\\n-----------------\\n1F: Convert different date string formats into datetime types passed: 4 out of 4 points\\n-----------------\\n1G: Convert transaction_date and posted_date columns to datetime types passed: 3 out of 3 points\\n-----------------\\n1H: Convert amount column to float type passed: 3 out of 3 points\\n-----------------\\n1I: Clean up text (string) columns passed: 3 out of 3 points\\n-----------------\\n2A: Filter 2014 transactions passed: 2 out of 2 points\\n-----------------\\n2B: Create a concatenated unique_name column passed: 2 out of 2 points\\n-----------------\\n2C: Create a month column passed: 2 out of 2 points\\n-----------------\\n2D: Remove unused columns passed: 2 out of 2 points\\n-----------------\\nAnalysis 1: Most expensive transactions by month in 2014 passed: 8 out of 8 points\\n-----------------\\nAnalysis 2: Cardholders with the largest number of transactions in 2014 passed: 6 out of 6 points\\n-----------------\\nAnalysis 3: Total amount spent by Merchant Category Code passed: 6 out of 6 points\\n-----------------\\nICT 1: Employee monthly spending limit passed: 6 out of 6 points\\n-----------------\\nICT 2: Splitting a large purchase into multiple smaller transactions passed: 7 out of 7 points\\n-----------------\\nFT 1: Benford Analysis passed: 7 out of 7 points\\n-----------------\\nFT 2A: Duplicate Transactions passed: 6 out of 6 points\\n-----------------\\nFT 2B: Duplicate Transactions (Worst Offenders) failed: 0 out of 6 points\\n[Autograder Output]\\nAssertionError: DataFrame.iloc[:, 0] (column name=\"unique_name\") are different\\n\\nDataFrame.iloc[:, 0] (column name=\"unique_name\") values are different (20.0 %)\\n[index]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\\n[left]:  [bailey, j, gebhart, g, knott, s, hines, g, tornakian, m, sisney, d, robinson, s, davis, e, roaseau, k, bryans, m]\\n[right]: [bailey, j, gebhart, g, knott, s, hines, g, sisney, d, tornakian, m, robinson, s, davis, e, roaseau, k, bryans, m]\\n\\n'}\n",
      "=============================\n",
      "Running ../../accy575-sp-2023/02-pcard-submissions\\excellent_65151_7725650_PCard_Excellent.ipynb successful\n",
      "Stored graded result as JSON to ../../accy575-sp-2023/02-pcard-submissions\\excellent_65151_7725650_PCard_Excellent-result.json\n",
      "{'filename': 'excellent_65151_7725650_PCard_Excellent-graded.ipynb', 'grading_finished_at': '2023-02-18T05:23:22.571158', 'grading_duration_in_seconds': 21.37, 'learner_score': 82, 'total_available': 82, 'num_test_cases': 20, 'num_passed_cases': 20, 'num_failed_cases': 0, 'summary': 'File: excellent_65151_7725650_PCard_Excellent-graded.ipynb\\nScore: 82 out of 82\\nPassed 20 out of 20 test cases\\nGrading took 21.37 seconds\\n\\nTest Case Summary\\n-----------------\\n1B: Create a list of column names passed: 2 out of 2 points\\n-----------------\\n1C: Read the CSV files into Pandas DataFrames passed: 3 out of 3 points\\n-----------------\\n1D: Filter only relevant data passed: 2 out of 2 points\\n-----------------\\n1E: Concatenate all transactions passed: 2 out of 2 points\\n-----------------\\n1F: Convert different date string formats into datetime types passed: 4 out of 4 points\\n-----------------\\n1G: Convert transaction_date and posted_date columns to datetime types passed: 3 out of 3 points\\n-----------------\\n1H: Convert amount column to float type passed: 3 out of 3 points\\n-----------------\\n1I: Clean up text (string) columns passed: 3 out of 3 points\\n-----------------\\n2A: Filter 2014 transactions passed: 2 out of 2 points\\n-----------------\\n2B: Create a concatenated unique_name column passed: 2 out of 2 points\\n-----------------\\n2C: Create a month column passed: 2 out of 2 points\\n-----------------\\n2D: Remove unused columns passed: 2 out of 2 points\\n-----------------\\nAnalysis 1: Most expensive transactions by month in 2014 passed: 8 out of 8 points\\n-----------------\\nAnalysis 2: Cardholders with the largest number of transactions in 2014 passed: 6 out of 6 points\\n-----------------\\nAnalysis 3: Total amount spent by Merchant Category Code passed: 6 out of 6 points\\n-----------------\\nICT 1: Employee monthly spending limit passed: 6 out of 6 points\\n-----------------\\nICT 2: Splitting a large purchase into multiple smaller transactions passed: 7 out of 7 points\\n-----------------\\nFT 1: Benford Analysis passed: 7 out of 7 points\\n-----------------\\nFT 2A: Duplicate Transactions passed: 6 out of 6 points\\n-----------------\\nFT 2B: Duplicate Transactions (Worst Offenders) passed: 6 out of 6 points\\n'}\n",
      "=============================\n",
      "Running ../../accy575-sp-2023/02-pcard-submissions\\get_an_a_323576_7703657_PCard-get_an_A.ipynb successful\n",
      "Stored graded result as JSON to ../../accy575-sp-2023/02-pcard-submissions\\get_an_a_323576_7703657_PCard-get_an_A-result.json\n",
      "{'filename': 'get_an_a_323576_7703657_PCard-get_an_A-graded.ipynb', 'grading_finished_at': '2023-02-18T05:23:46.951679', 'grading_duration_in_seconds': 21.14, 'learner_score': 82, 'total_available': 82, 'num_test_cases': 20, 'num_passed_cases': 20, 'num_failed_cases': 0, 'summary': 'File: get_an_a_323576_7703657_PCard-get_an_A-graded.ipynb\\nScore: 82 out of 82\\nPassed 20 out of 20 test cases\\nGrading took 21.14 seconds\\n\\nTest Case Summary\\n-----------------\\n1B: Create a list of column names passed: 2 out of 2 points\\n-----------------\\n1C: Read the CSV files into Pandas DataFrames passed: 3 out of 3 points\\n-----------------\\n1D: Filter only relevant data passed: 2 out of 2 points\\n-----------------\\n1E: Concatenate all transactions passed: 2 out of 2 points\\n-----------------\\n1F: Convert different date string formats into datetime types passed: 4 out of 4 points\\n-----------------\\n1G: Convert transaction_date and posted_date columns to datetime types passed: 3 out of 3 points\\n-----------------\\n1H: Convert amount column to float type passed: 3 out of 3 points\\n-----------------\\n1I: Clean up text (string) columns passed: 3 out of 3 points\\n-----------------\\n2A: Filter 2014 transactions passed: 2 out of 2 points\\n-----------------\\n2B: Create a concatenated unique_name column passed: 2 out of 2 points\\n-----------------\\n2C: Create a month column passed: 2 out of 2 points\\n-----------------\\n2D: Remove unused columns passed: 2 out of 2 points\\n-----------------\\nAnalysis 1: Most expensive transactions by month in 2014 passed: 8 out of 8 points\\n-----------------\\nAnalysis 2: Cardholders with the largest number of transactions in 2014 passed: 6 out of 6 points\\n-----------------\\nAnalysis 3: Total amount spent by Merchant Category Code passed: 6 out of 6 points\\n-----------------\\nICT 1: Employee monthly spending limit passed: 6 out of 6 points\\n-----------------\\nICT 2: Splitting a large purchase into multiple smaller transactions passed: 7 out of 7 points\\n-----------------\\nFT 1: Benford Analysis passed: 7 out of 7 points\\n-----------------\\nFT 2A: Duplicate Transactions passed: 6 out of 6 points\\n-----------------\\nFT 2B: Duplicate Transactions (Worst Offenders) passed: 6 out of 6 points\\n'}\n",
      "=============================\n",
      "Running ../../accy575-sp-2023/02-pcard-submissions\\group_x_6126_7702498_PCard-after-milestone_Group_X .ipynb successful\n",
      "Stored graded result as JSON to ../../accy575-sp-2023/02-pcard-submissions\\group_x_6126_7702498_PCard-after-milestone_Group_X -result.json\n",
      "{'filename': 'group_x_6126_7702498_PCard-after-milestone_Group_X -graded.ipynb', 'grading_finished_at': '2023-02-18T05:24:11.332804', 'grading_duration_in_seconds': 21.22, 'learner_score': 82, 'total_available': 82, 'num_test_cases': 20, 'num_passed_cases': 20, 'num_failed_cases': 0, 'summary': 'File: group_x_6126_7702498_PCard-after-milestone_Group_X -graded.ipynb\\nScore: 82 out of 82\\nPassed 20 out of 20 test cases\\nGrading took 21.22 seconds\\n\\nTest Case Summary\\n-----------------\\n1B: Create a list of column names passed: 2 out of 2 points\\n-----------------\\n1C: Read the CSV files into Pandas DataFrames passed: 3 out of 3 points\\n-----------------\\n1D: Filter only relevant data passed: 2 out of 2 points\\n-----------------\\n1E: Concatenate all transactions passed: 2 out of 2 points\\n-----------------\\n1F: Convert different date string formats into datetime types passed: 4 out of 4 points\\n-----------------\\n1G: Convert transaction_date and posted_date columns to datetime types passed: 3 out of 3 points\\n-----------------\\n1H: Convert amount column to float type passed: 3 out of 3 points\\n-----------------\\n1I: Clean up text (string) columns passed: 3 out of 3 points\\n-----------------\\n2A: Filter 2014 transactions passed: 2 out of 2 points\\n-----------------\\n2B: Create a concatenated unique_name column passed: 2 out of 2 points\\n-----------------\\n2C: Create a month column passed: 2 out of 2 points\\n-----------------\\n2D: Remove unused columns passed: 2 out of 2 points\\n-----------------\\nAnalysis 1: Most expensive transactions by month in 2014 passed: 8 out of 8 points\\n-----------------\\nAnalysis 2: Cardholders with the largest number of transactions in 2014 passed: 6 out of 6 points\\n-----------------\\nAnalysis 3: Total amount spent by Merchant Category Code passed: 6 out of 6 points\\n-----------------\\nICT 1: Employee monthly spending limit passed: 6 out of 6 points\\n-----------------\\nICT 2: Splitting a large purchase into multiple smaller transactions passed: 7 out of 7 points\\n-----------------\\nFT 1: Benford Analysis passed: 7 out of 7 points\\n-----------------\\nFT 2A: Duplicate Transactions passed: 6 out of 6 points\\n-----------------\\nFT 2B: Duplicate Transactions (Worst Offenders) passed: 6 out of 6 points\\n'}\n",
      "=============================\n",
      "Running ../../accy575-sp-2023/02-pcard-submissions\\may_323193_7713466_PCard7.ipynb successful\n",
      "Stored graded result as JSON to ../../accy575-sp-2023/02-pcard-submissions\\may_323193_7713466_PCard7-result.json\n",
      "{'filename': 'may_323193_7713466_PCard7-graded.ipynb', 'grading_finished_at': '2023-02-18T05:24:47.022665', 'grading_duration_in_seconds': 32.37, 'learner_score': 67, 'total_available': 82, 'num_test_cases': 20, 'num_passed_cases': 18, 'num_failed_cases': 2, 'summary': 'File: may_323193_7713466_PCard7-graded.ipynb\\nScore: 67 out of 82\\nPassed 18 out of 20 test cases\\nGrading took 32.37 seconds\\n\\nTest Case Summary\\n-----------------\\n1B: Create a list of column names passed: 2 out of 2 points\\n-----------------\\n1C: Read the CSV files into Pandas DataFrames passed: 3 out of 3 points\\n-----------------\\n1D: Filter only relevant data passed: 2 out of 2 points\\n-----------------\\n1E: Concatenate all transactions passed: 2 out of 2 points\\n-----------------\\n1F: Convert different date string formats into datetime types passed: 4 out of 4 points\\n-----------------\\n1G: Convert transaction_date and posted_date columns to datetime types passed: 3 out of 3 points\\n-----------------\\n1H: Convert amount column to float type passed: 3 out of 3 points\\n-----------------\\n1I: Clean up text (string) columns passed: 3 out of 3 points\\n-----------------\\n2A: Filter 2014 transactions passed: 2 out of 2 points\\n-----------------\\n2B: Create a concatenated unique_name column passed: 2 out of 2 points\\n-----------------\\n2C: Create a month column passed: 2 out of 2 points\\n-----------------\\n2D: Remove unused columns passed: 2 out of 2 points\\n-----------------\\nAnalysis 1: Most expensive transactions by month in 2014 failed: 0 out of 8 points\\n[Autograder Output]\\nTypeError: unsupported operand type(s) for -: \\'list\\' and \\'list\\'\\n\\n-----------------\\nAnalysis 2: Cardholders with the largest number of transactions in 2014 passed: 6 out of 6 points\\n-----------------\\nAnalysis 3: Total amount spent by Merchant Category Code passed: 6 out of 6 points\\n-----------------\\nICT 1: Employee monthly spending limit passed: 6 out of 6 points\\n-----------------\\nICT 2: Splitting a large purchase into multiple smaller transactions passed: 7 out of 7 points\\n-----------------\\nFT 1: Benford Analysis failed: 0 out of 7 points\\n[Autograder Output]\\nAssertionError: DataFrame.iloc[:, 2] (column name=\"percent\") are different\\n\\nDataFrame.iloc[:, 2] (column name=\"percent\") values are different (100.0 %)\\n[index]: [0, 1, 2, 3, 4, 5, 6, 7, 8]\\n[left]:  [29.528268236133403, 17.77064383805957, 12.866952024255395, 9.813625824861779, 8.220973782771535, 6.124487248082754, 5.512751917246299, 5.371856607811664, 4.790440520777599]\\n[right]: [0.29528268236133404, 0.1777064383805957, 0.12866952024255396, 0.0981362582486178, 0.08220973782771536, 0.061244872480827536, 0.055127519172462995, 0.05371856607811664, 0.047904405207775995]\\n\\n-----------------\\nFT 2A: Duplicate Transactions passed: 6 out of 6 points\\n-----------------\\nFT 2B: Duplicate Transactions (Worst Offenders) passed: 6 out of 6 points\\n'}\n",
      "=============================\n",
      "Running ../../accy575-sp-2023/02-pcard-submissions\\numpy_321946_7726318_Group Numpy_Case 2 P-Card Python Portion.ipynb successful\n",
      "Stored graded result as JSON to ../../accy575-sp-2023/02-pcard-submissions\\numpy_321946_7726318_Group Numpy_Case 2 P-Card Python Portion-result.json\n",
      "{'filename': 'numpy_321946_7726318_Group Numpy_Case 2 P-Card Python Portion-graded.ipynb', 'grading_finished_at': '2023-02-18T05:25:13.768791', 'grading_duration_in_seconds': 22.62, 'learner_score': 57, 'total_available': 82, 'num_test_cases': 20, 'num_passed_cases': 16, 'num_failed_cases': 4, 'summary': 'File: numpy_321946_7726318_Group Numpy_Case 2 P-Card Python Portion-graded.ipynb\\nScore: 57 out of 82\\nPassed 16 out of 20 test cases\\nGrading took 22.62 seconds\\n\\nTest Case Summary\\n-----------------\\n1B: Create a list of column names passed: 2 out of 2 points\\n-----------------\\n1C: Read the CSV files into Pandas DataFrames passed: 3 out of 3 points\\n-----------------\\n1D: Filter only relevant data passed: 2 out of 2 points\\n-----------------\\n1E: Concatenate all transactions passed: 2 out of 2 points\\n-----------------\\n1F: Convert different date string formats into datetime types passed: 4 out of 4 points\\n-----------------\\n1G: Convert transaction_date and posted_date columns to datetime types passed: 3 out of 3 points\\n-----------------\\n1H: Convert amount column to float type passed: 3 out of 3 points\\n-----------------\\n1I: Clean up text (string) columns passed: 3 out of 3 points\\n-----------------\\n2A: Filter 2014 transactions passed: 2 out of 2 points\\n-----------------\\n2B: Create a concatenated unique_name column passed: 2 out of 2 points\\n-----------------\\n2C: Create a month column passed: 2 out of 2 points\\n-----------------\\n2D: Remove unused columns passed: 2 out of 2 points\\n-----------------\\nAnalysis 1: Most expensive transactions by month in 2014 passed: 8 out of 8 points\\n-----------------\\nAnalysis 2: Cardholders with the largest number of transactions in 2014 failed: 0 out of 6 points\\n[Autograder Output]\\nAssertionError: DataFrame.columns are different\\n\\nDataFrame.columns values are different (50.0 %)\\n[left]:  Index([\\'unique_name\\', \\'amount\\'], dtype=\\'object\\')\\n[right]: Index([\\'unique_name\\', \\'num_transactions\\'], dtype=\\'object\\')\\n\\n-----------------\\nAnalysis 3: Total amount spent by Merchant Category Code passed: 6 out of 6 points\\n-----------------\\nICT 1: Employee monthly spending limit failed: 0 out of 6 points\\n[Autograder Output]\\nAssertionError: DataFrame.iloc[:, 0] (column name=\"unique_name\") are different\\n\\nDataFrame.iloc[:, 0] (column name=\"unique_name\") values are different (98.47162 %)\\n[index]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\\n[left]:  [abernathy, m, al-harake, m, al-harake, m, al-harake, m, al-harake, m, al-harake, m, allen, r, allen, r, arena, a, arena, a, arena, a, arnold, d, arnold, d, arnold, d, arthur, h, arthur, h, bailey, j, bailey, j, bailey, j, bailey, j, bailey, j, bailey, j, bailey, j, bailey, j, bailey, j, bailey, j, bailey, j, baker, b, baker, b, ballard, j, bateson, d, biggs, j, biggs, j, biggs, j, biggs, j, biggs, j, biggs, j, biggs, j, bilbeisi, s, bishop, k, blakley, t, blakley, t, blakley, t, blakley, t, blakley, t, blakley, t, boettcher, c, boettcher, c, boettcher, c, boettcher, c, boettcher, c, boettcher, c, boettcher, c, boettcher, c, bowers, r, bowers, r, bowers, r, bowers, r, bowers, r, bowers, r, bowers, r, bowers, r, bowers, r, bowers, r, bowers, r, bowers, r, boyd, k, brinker, b, brinker, b, brinker, b, brinker, b, brinker, b, brown, g, bryant, k, bryant, k, bryant, k, bryant, k, bryant, k, budd, c, burns, r, burruss, j, burruss, j, burruss, j, burruss, j, buser, m, byrd, j, campbell, d, carter, a, carter, a, chang, y, chapman, m, chapman, m, chapman, m, chapman, m, chapman, m, chowdhary, g, chowdhary, g, clark, c, clark, c, clark, c, ...]\\n[right]: [hines, g, hines, g, hines, g, hines, g, hines, g, hines, g, hines, g, hines, g, tornakian, m, hines, g, tornakian, m, tornakian, m, tornakian, m, heusel, j, hines, g, tornakian, m, heusel, j, tornakian, m, heusel, j, heusel, j, tornakian, m, heusel, j, tornakian, m, munday, t, tornakian, m, tornakian, m, tornakian, m, munday, t, forquer, h, heusel, j, hines, g, fitzpatrick, s, tornakian, m, forquer, h, munday, t, hines, g, fitzpatrick, s, fitzpatrick, s, munday, t, ropers, a, bowers, r, teel, j, kindschi, j, fitzpatrick, s, stover, c, yarbrough-tessman, v, bowers, r, fitzpatrick, s, yarbrough-tessman, v, whitmore, d, connelly, d, bowers, r, clark, m, biggs, j, bowers, r, clark, m, teel, j, bowers, r, bowers, r, stover, c, stover, c, fitzpatrick, s, bowers, r, turner, d, connelly, d, fitzpatrick, s, turner, d, stover, c, munday, t, wilhelm, m, fitzpatrick, s, wright, b, clark, c, marshall, s, clark, m, bowers, r, heusel, j, ropers, a, teel, j, yarbrough-tessman, v, fitzpatrick, s, boettcher, c, yarbrough-tessman, v, brinker, b, whitmore, d, forquer, h, sabins, t, bailey, j, bowers, r, yarbrough-tessman, v, clark, m, ropers, a, yarbrough-tessman, v, wood, c, turner, d, whitefield, d, bowers, r, clark, m, teel, j, yarbrough-tessman, v, ...]\\n\\n-----------------\\nICT 2: Splitting a large purchase into multiple smaller transactions passed: 7 out of 7 points\\n-----------------\\nFT 1: Benford Analysis failed: 0 out of 7 points\\n[Autograder Output]\\nAssertionError: DataFrame.columns are different\\n\\nDataFrame.columns values are different (33.33333 %)\\n[left]:  Index([\\'first_digit\\', \\'count\\', \\'percentage\\'], dtype=\\'object\\')\\n[right]: Index([\\'first_digit\\', \\'count\\', \\'percent\\'], dtype=\\'object\\')\\n\\n-----------------\\nFT 2A: Duplicate Transactions passed: 6 out of 6 points\\n-----------------\\nFT 2B: Duplicate Transactions (Worst Offenders) failed: 0 out of 6 points\\n[Autograder Output]\\nAssertionError: DataFrame.iloc[:, 0] (column name=\"unique_name\") are different\\n\\nDataFrame.iloc[:, 0] (column name=\"unique_name\") values are different (70.0 %)\\n[index]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\\n[left]:  [bailey, j, roaseau, k, knott, s, gebhart, g, sisney, d, bryans, m, hines, g, tornakian, m, davis, e, robinson, s]\\n[right]: [bailey, j, gebhart, g, knott, s, hines, g, sisney, d, tornakian, m, robinson, s, davis, e, roaseau, k, bryans, m]\\n\\n'}\n",
      "=============================\n",
      "Running ../../accy575-sp-2023/02-pcard-submissions\\pandas_330521_7726659_Pandas_PCard.ipynb successful\n",
      "Stored graded result as JSON to ../../accy575-sp-2023/02-pcard-submissions\\pandas_330521_7726659_Pandas_PCard-result.json\n",
      "{'filename': 'pandas_330521_7726659_Pandas_PCard-graded.ipynb', 'grading_finished_at': '2023-02-18T05:25:39.494973', 'grading_duration_in_seconds': 21.85, 'learner_score': 76, 'total_available': 82, 'num_test_cases': 20, 'num_passed_cases': 19, 'num_failed_cases': 1, 'summary': 'File: pandas_330521_7726659_Pandas_PCard-graded.ipynb\\nScore: 76 out of 82\\nPassed 19 out of 20 test cases\\nGrading took 21.85 seconds\\n\\nTest Case Summary\\n-----------------\\n1B: Create a list of column names passed: 2 out of 2 points\\n-----------------\\n1C: Read the CSV files into Pandas DataFrames passed: 3 out of 3 points\\n-----------------\\n1D: Filter only relevant data passed: 2 out of 2 points\\n-----------------\\n1E: Concatenate all transactions passed: 2 out of 2 points\\n-----------------\\n1F: Convert different date string formats into datetime types passed: 4 out of 4 points\\n-----------------\\n1G: Convert transaction_date and posted_date columns to datetime types passed: 3 out of 3 points\\n-----------------\\n1H: Convert amount column to float type passed: 3 out of 3 points\\n-----------------\\n1I: Clean up text (string) columns passed: 3 out of 3 points\\n-----------------\\n2A: Filter 2014 transactions passed: 2 out of 2 points\\n-----------------\\n2B: Create a concatenated unique_name column passed: 2 out of 2 points\\n-----------------\\n2C: Create a month column passed: 2 out of 2 points\\n-----------------\\n2D: Remove unused columns passed: 2 out of 2 points\\n-----------------\\nAnalysis 1: Most expensive transactions by month in 2014 passed: 8 out of 8 points\\n-----------------\\nAnalysis 2: Cardholders with the largest number of transactions in 2014 passed: 6 out of 6 points\\n-----------------\\nAnalysis 3: Total amount spent by Merchant Category Code passed: 6 out of 6 points\\n-----------------\\nICT 1: Employee monthly spending limit passed: 6 out of 6 points\\n-----------------\\nICT 2: Splitting a large purchase into multiple smaller transactions passed: 7 out of 7 points\\n-----------------\\nFT 1: Benford Analysis passed: 7 out of 7 points\\n-----------------\\nFT 2A: Duplicate Transactions passed: 6 out of 6 points\\n-----------------\\nFT 2B: Duplicate Transactions (Worst Offenders) failed: 0 out of 6 points\\n[Autograder Output]\\nAssertionError: DataFrame.iloc[:, 0] (column name=\"unique_name\") are different\\n\\nDataFrame.iloc[:, 0] (column name=\"unique_name\") values are different (70.0 %)\\n[index]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\\n[left]:  [bailey, j, roaseau, k, knott, s, gebhart, g, sisney, d, bryans, m, hines, g, tornakian, m, davis, e, robinson, s]\\n[right]: [bailey, j, gebhart, g, knott, s, hines, g, sisney, d, tornakian, m, robinson, s, davis, e, roaseau, k, bryans, m]\\n\\n'}\n",
      "=============================\n",
      "Running ../../accy575-sp-2023/02-pcard-submissions\\pythons_3369_7684411_PCard-after-milestone.ipynb successful\n",
      "Stored graded result as JSON to ../../accy575-sp-2023/02-pcard-submissions\\pythons_3369_7684411_PCard-after-milestone-result.json\n",
      "{'filename': 'pythons_3369_7684411_PCard-after-milestone-graded.ipynb', 'grading_finished_at': '2023-02-18T05:26:05.382034', 'grading_duration_in_seconds': 22.05, 'learner_score': 63, 'total_available': 82, 'num_test_cases': 20, 'num_passed_cases': 17, 'num_failed_cases': 3, 'summary': 'File: pythons_3369_7684411_PCard-after-milestone-graded.ipynb\\nScore: 63 out of 82\\nPassed 17 out of 20 test cases\\nGrading took 22.05 seconds\\n\\nTest Case Summary\\n-----------------\\n1B: Create a list of column names passed: 2 out of 2 points\\n-----------------\\n1C: Read the CSV files into Pandas DataFrames passed: 3 out of 3 points\\n-----------------\\n1D: Filter only relevant data passed: 2 out of 2 points\\n-----------------\\n1E: Concatenate all transactions passed: 2 out of 2 points\\n-----------------\\n1F: Convert different date string formats into datetime types passed: 4 out of 4 points\\n-----------------\\n1G: Convert transaction_date and posted_date columns to datetime types passed: 3 out of 3 points\\n-----------------\\n1H: Convert amount column to float type passed: 3 out of 3 points\\n-----------------\\n1I: Clean up text (string) columns passed: 3 out of 3 points\\n-----------------\\n2A: Filter 2014 transactions passed: 2 out of 2 points\\n-----------------\\n2B: Create a concatenated unique_name column passed: 2 out of 2 points\\n-----------------\\n2C: Create a month column passed: 2 out of 2 points\\n-----------------\\n2D: Remove unused columns passed: 2 out of 2 points\\n-----------------\\nAnalysis 1: Most expensive transactions by month in 2014 passed: 8 out of 8 points\\n-----------------\\nAnalysis 2: Cardholders with the largest number of transactions in 2014 passed: 6 out of 6 points\\n-----------------\\nAnalysis 3: Total amount spent by Merchant Category Code passed: 6 out of 6 points\\n-----------------\\nICT 1: Employee monthly spending limit passed: 6 out of 6 points\\n-----------------\\nICT 2: Splitting a large purchase into multiple smaller transactions passed: 7 out of 7 points\\n-----------------\\nFT 1: Benford Analysis failed: 0 out of 7 points\\n[Autograder Output]\\nAssertionError: Attributes of DataFrame.iloc[:, 1] (column name=\"count\") are different\\n\\nAttribute \"dtype\" are different\\n[left]:  float64\\n[right]: int64\\n\\n-----------------\\nFT 2A: Duplicate Transactions failed: 0 out of 6 points\\n[Autograder Output]\\nAssertionError: DataFrame.iloc[:, 4] (column name=\"duplicate_count\") are different\\n\\nDataFrame.iloc[:, 4] (column name=\"duplicate_count\") values are different (100.0 %)\\n[index]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\\n[left]:  [1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 4, 2, 3, 1, 2, 11, 7, 2, 2, 3, 1, 1, 1, 2, 1, 1, 10, 2, 17, 10, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 5, 1, 1, 3, 1, 3, 1, 1, 1, 1, 6, 3, 2, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 5, 1, 1, 4, 8, 1, 2, 1, 2, 3, 1, 3, 2, 3, 1, 1, 1, ...]\\n[right]: [2, 5, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 5, 3, 4, 2, 3, 12, 8, 3, 3, 4, 2, 2, 2, 3, 2, 2, 11, 3, 18, 11, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 6, 2, 2, 4, 2, 4, 2, 2, 2, 2, 7, 4, 3, 2, 3, 2, 4, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 6, 2, 2, 5, 9, 2, 3, 2, 3, 4, 2, 4, 3, 4, 2, 2, 2, ...]\\n\\n-----------------\\nFT 2B: Duplicate Transactions (Worst Offenders) failed: 0 out of 6 points\\n[Autograder Output]\\nAssertionError: DataFrame.iloc[:, 0] (column name=\"unique_name\") are different\\n\\nDataFrame.iloc[:, 0] (column name=\"unique_name\") values are different (80.0 %)\\n[index]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\\n[left]:  [roaseau, k, bailey, j, knott, s, gebhart, g, sisney, d, bryans, m, hines, g, tornakian, m, chapman, m, longan, s]\\n[right]: [bailey, j, gebhart, g, knott, s, hines, g, sisney, d, tornakian, m, robinson, s, davis, e, roaseau, k, bryans, m]\\n\\n'}\n",
      "=============================\n",
      "Running ../../accy575-sp-2023/02-pcard-submissions\\solid_312038_7711757_PCard_Solid.ipynb successful\n",
      "Stored graded result as JSON to ../../accy575-sp-2023/02-pcard-submissions\\solid_312038_7711757_PCard_Solid-result.json\n",
      "{'filename': 'solid_312038_7711757_PCard_Solid-graded.ipynb', 'grading_finished_at': '2023-02-18T05:27:02.055952', 'grading_duration_in_seconds': 52.79, 'learner_score': 82, 'total_available': 82, 'num_test_cases': 20, 'num_passed_cases': 20, 'num_failed_cases': 0, 'summary': 'File: solid_312038_7711757_PCard_Solid-graded.ipynb\\nScore: 82 out of 82\\nPassed 20 out of 20 test cases\\nGrading took 52.79 seconds\\n\\nTest Case Summary\\n-----------------\\n1B: Create a list of column names passed: 2 out of 2 points\\n-----------------\\n1C: Read the CSV files into Pandas DataFrames passed: 3 out of 3 points\\n-----------------\\n1D: Filter only relevant data passed: 2 out of 2 points\\n-----------------\\n1E: Concatenate all transactions passed: 2 out of 2 points\\n-----------------\\n1F: Convert different date string formats into datetime types passed: 4 out of 4 points\\n-----------------\\n1G: Convert transaction_date and posted_date columns to datetime types passed: 3 out of 3 points\\n-----------------\\n1H: Convert amount column to float type passed: 3 out of 3 points\\n-----------------\\n1I: Clean up text (string) columns passed: 3 out of 3 points\\n-----------------\\n2A: Filter 2014 transactions passed: 2 out of 2 points\\n-----------------\\n2B: Create a concatenated unique_name column passed: 2 out of 2 points\\n-----------------\\n2C: Create a month column passed: 2 out of 2 points\\n-----------------\\n2D: Remove unused columns passed: 2 out of 2 points\\n-----------------\\nAnalysis 1: Most expensive transactions by month in 2014 passed: 8 out of 8 points\\n-----------------\\nAnalysis 2: Cardholders with the largest number of transactions in 2014 passed: 6 out of 6 points\\n-----------------\\nAnalysis 3: Total amount spent by Merchant Category Code passed: 6 out of 6 points\\n-----------------\\nICT 1: Employee monthly spending limit passed: 6 out of 6 points\\n-----------------\\nICT 2: Splitting a large purchase into multiple smaller transactions passed: 7 out of 7 points\\n-----------------\\nFT 1: Benford Analysis passed: 7 out of 7 points\\n-----------------\\nFT 2A: Duplicate Transactions passed: 6 out of 6 points\\n-----------------\\nFT 2B: Duplicate Transactions (Worst Offenders) passed: 6 out of 6 points\\n'}\n",
      "=============================\n",
      "Running ../../accy575-sp-2023/02-pcard-submissions\\square_328074_7722967_PCard form Square .ipynb successful\n",
      "Stored graded result as JSON to ../../accy575-sp-2023/02-pcard-submissions\\square_328074_7722967_PCard form Square -result.json\n",
      "{'filename': 'square_328074_7722967_PCard form Square -graded.ipynb', 'grading_finished_at': '2023-02-18T05:27:29.941621', 'grading_duration_in_seconds': 24.32, 'learner_score': 75, 'total_available': 82, 'num_test_cases': 20, 'num_passed_cases': 19, 'num_failed_cases': 1, 'summary': \"File: square_328074_7722967_PCard form Square -graded.ipynb\\nScore: 75 out of 82\\nPassed 19 out of 20 test cases\\nGrading took 24.32 seconds\\n\\nTest Case Summary\\n-----------------\\n1B: Create a list of column names passed: 2 out of 2 points\\n-----------------\\n1C: Read the CSV files into Pandas DataFrames passed: 3 out of 3 points\\n-----------------\\n1D: Filter only relevant data passed: 2 out of 2 points\\n-----------------\\n1E: Concatenate all transactions passed: 2 out of 2 points\\n-----------------\\n1F: Convert different date string formats into datetime types passed: 4 out of 4 points\\n-----------------\\n1G: Convert transaction_date and posted_date columns to datetime types passed: 3 out of 3 points\\n-----------------\\n1H: Convert amount column to float type passed: 3 out of 3 points\\n-----------------\\n1I: Clean up text (string) columns passed: 3 out of 3 points\\n-----------------\\n2A: Filter 2014 transactions passed: 2 out of 2 points\\n-----------------\\n2B: Create a concatenated unique_name column passed: 2 out of 2 points\\n-----------------\\n2C: Create a month column passed: 2 out of 2 points\\n-----------------\\n2D: Remove unused columns passed: 2 out of 2 points\\n-----------------\\nAnalysis 1: Most expensive transactions by month in 2014 passed: 8 out of 8 points\\n-----------------\\nAnalysis 2: Cardholders with the largest number of transactions in 2014 passed: 6 out of 6 points\\n-----------------\\nAnalysis 3: Total amount spent by Merchant Category Code passed: 6 out of 6 points\\n-----------------\\nICT 1: Employee monthly spending limit passed: 6 out of 6 points\\n-----------------\\nICT 2: Splitting a large purchase into multiple smaller transactions failed: 0 out of 7 points\\n[Autograder Output]\\nAssertionError: DataFrame.columns are different\\n\\nDataFrame.columns values are different (40.0 %)\\n[left]:  Index(['unique_name', 'vendor', 'transaction_date', 'amount', 'count'], dtype='object')\\n[right]: Index(['unique_name', 'vendor', 'transaction_date', 'count', 'amount'], dtype='object')\\n\\n-----------------\\nFT 1: Benford Analysis passed: 7 out of 7 points\\n-----------------\\nFT 2A: Duplicate Transactions passed: 6 out of 6 points\\n-----------------\\nFT 2B: Duplicate Transactions (Worst Offenders) passed: 6 out of 6 points\\n\"}\n",
      "=============================\n",
      "Running ../../accy575-sp-2023/02-pcard-submissions\\teletubbies_296476_7725249_PCard_Teletubbies-2.ipynb successful\n",
      "Stored graded result as JSON to ../../accy575-sp-2023/02-pcard-submissions\\teletubbies_296476_7725249_PCard_Teletubbies-2-result.json\n",
      "{'filename': 'teletubbies_296476_7725249_PCard_Teletubbies-2-graded.ipynb', 'grading_finished_at': '2023-02-18T05:27:55.458520', 'grading_duration_in_seconds': 21.69, 'learner_score': 75, 'total_available': 82, 'num_test_cases': 20, 'num_passed_cases': 19, 'num_failed_cases': 1, 'summary': 'File: teletubbies_296476_7725249_PCard_Teletubbies-2-graded.ipynb\\nScore: 75 out of 82\\nPassed 19 out of 20 test cases\\nGrading took 21.69 seconds\\n\\nTest Case Summary\\n-----------------\\n1B: Create a list of column names passed: 2 out of 2 points\\n-----------------\\n1C: Read the CSV files into Pandas DataFrames passed: 3 out of 3 points\\n-----------------\\n1D: Filter only relevant data passed: 2 out of 2 points\\n-----------------\\n1E: Concatenate all transactions passed: 2 out of 2 points\\n-----------------\\n1F: Convert different date string formats into datetime types passed: 4 out of 4 points\\n-----------------\\n1G: Convert transaction_date and posted_date columns to datetime types passed: 3 out of 3 points\\n-----------------\\n1H: Convert amount column to float type passed: 3 out of 3 points\\n-----------------\\n1I: Clean up text (string) columns passed: 3 out of 3 points\\n-----------------\\n2A: Filter 2014 transactions passed: 2 out of 2 points\\n-----------------\\n2B: Create a concatenated unique_name column passed: 2 out of 2 points\\n-----------------\\n2C: Create a month column passed: 2 out of 2 points\\n-----------------\\n2D: Remove unused columns passed: 2 out of 2 points\\n-----------------\\nAnalysis 1: Most expensive transactions by month in 2014 passed: 8 out of 8 points\\n-----------------\\nAnalysis 2: Cardholders with the largest number of transactions in 2014 passed: 6 out of 6 points\\n-----------------\\nAnalysis 3: Total amount spent by Merchant Category Code passed: 6 out of 6 points\\n-----------------\\nICT 1: Employee monthly spending limit passed: 6 out of 6 points\\n-----------------\\nICT 2: Splitting a large purchase into multiple smaller transactions passed: 7 out of 7 points\\n-----------------\\nFT 1: Benford Analysis failed: 0 out of 7 points\\n[Autograder Output]\\nAssertionError: DataFrame.iloc[:, 2] (column name=\"percent\") are different\\n\\nDataFrame.iloc[:, 2] (column name=\"percent\") values are different (100.0 %)\\n[index]: [0, 1, 2, 3, 4, 5, 6, 7, 8]\\n[left]:  [29.528268236133403, 17.77064383805957, 12.866952024255395, 9.813625824861779, 8.220973782771535, 6.124487248082754, 5.512751917246299, 5.371856607811664, 4.790440520777599]\\n[right]: [0.29528268236133404, 0.1777064383805957, 0.12866952024255396, 0.0981362582486178, 0.08220973782771536, 0.061244872480827536, 0.055127519172462995, 0.05371856607811664, 0.047904405207775995]\\n\\n-----------------\\nFT 2A: Duplicate Transactions passed: 6 out of 6 points\\n-----------------\\nFT 2B: Duplicate Transactions (Worst Offenders) passed: 6 out of 6 points\\n'}\n",
      "=============================\n",
      "Running ../../accy575-sp-2023/02-pcard-submissions\\thosh_7733_7705784_PCard-after-milestone.ipynb successful\n",
      "Stored graded result as JSON to ../../accy575-sp-2023/02-pcard-submissions\\thosh_7733_7705784_PCard-after-milestone-result.json\n",
      "{'filename': 'thosh_7733_7705784_PCard-after-milestone-graded.ipynb', 'grading_finished_at': '2023-02-18T05:28:20.565526', 'grading_duration_in_seconds': 21.75, 'learner_score': 63, 'total_available': 82, 'num_test_cases': 20, 'num_passed_cases': 17, 'num_failed_cases': 3, 'summary': 'File: thosh_7733_7705784_PCard-after-milestone-graded.ipynb\\nScore: 63 out of 82\\nPassed 17 out of 20 test cases\\nGrading took 21.75 seconds\\n\\nTest Case Summary\\n-----------------\\n1B: Create a list of column names passed: 2 out of 2 points\\n-----------------\\n1C: Read the CSV files into Pandas DataFrames passed: 3 out of 3 points\\n-----------------\\n1D: Filter only relevant data passed: 2 out of 2 points\\n-----------------\\n1E: Concatenate all transactions passed: 2 out of 2 points\\n-----------------\\n1F: Convert different date string formats into datetime types passed: 4 out of 4 points\\n-----------------\\n1G: Convert transaction_date and posted_date columns to datetime types passed: 3 out of 3 points\\n-----------------\\n1H: Convert amount column to float type passed: 3 out of 3 points\\n-----------------\\n1I: Clean up text (string) columns passed: 3 out of 3 points\\n-----------------\\n2A: Filter 2014 transactions passed: 2 out of 2 points\\n-----------------\\n2B: Create a concatenated unique_name column passed: 2 out of 2 points\\n-----------------\\n2C: Create a month column passed: 2 out of 2 points\\n-----------------\\n2D: Remove unused columns passed: 2 out of 2 points\\n-----------------\\nAnalysis 1: Most expensive transactions by month in 2014 passed: 8 out of 8 points\\n-----------------\\nAnalysis 2: Cardholders with the largest number of transactions in 2014 passed: 6 out of 6 points\\n-----------------\\nAnalysis 3: Total amount spent by Merchant Category Code passed: 6 out of 6 points\\n-----------------\\nICT 1: Employee monthly spending limit passed: 6 out of 6 points\\n-----------------\\nICT 2: Splitting a large purchase into multiple smaller transactions passed: 7 out of 7 points\\n-----------------\\nFT 1: Benford Analysis failed: 0 out of 7 points\\n[Autograder Output]\\nAssertionError: DataFrame.columns are different\\n\\nDataFrame.columns values are different (33.33333 %)\\n[left]:  Index([\\'first_digit\\', \\'count\\', \\'percentage\\'], dtype=\\'object\\')\\n[right]: Index([\\'first_digit\\', \\'count\\', \\'percent\\'], dtype=\\'object\\')\\n\\n-----------------\\nFT 2A: Duplicate Transactions failed: 0 out of 6 points\\n[Autograder Output]\\nAssertionError: 1842 != 1733\\n\\n-----------------\\nFT 2B: Duplicate Transactions (Worst Offenders) failed: 0 out of 6 points\\n[Autograder Output]\\nAssertionError: DataFrame.iloc[:, 0] (column name=\"unique_name\") are different\\n\\nDataFrame.iloc[:, 0] (column name=\"unique_name\") values are different (100.0 %)\\n[index]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\\n[left]:  [gordon, d, haseley, s, bishop, k, roaseau, k, clark, k, longan, s, roaseau, k, robinson, d, simmons, s, sanderson, d]\\n[right]: [bailey, j, gebhart, g, knott, s, hines, g, sisney, d, tornakian, m, robinson, s, davis, e, roaseau, k, bryans, m]\\n\\n'}\n"
     ]
    }
   ],
   "source": [
    "from lambdagrader import *\n",
    "\n",
    "graded_results = []\n",
    "\n",
    "for notebook_path in fpaths:\n",
    "    try:\n",
    "        print('=============================')\n",
    "        nb = nbformat.read(notebook_path, as_version=4)\n",
    "        \n",
    "        for cell in nb.cells:\n",
    "            test_case_metadata = extract_test_case_metadata_from_cell(cell.source)\n",
    "\n",
    "            if test_case_metadata:\n",
    "                cell.source = convert_to_grader_code(cell.source)\n",
    "\n",
    "        add_scripts_to_notebook(nb)\n",
    "        \n",
    "        ep = ExecutePreprocessor(\n",
    "            timeout=1800,\n",
    "            kernel_name='python3',\n",
    "            allow_errors=True\n",
    "        )\n",
    "        ep.preprocess(nb)\n",
    "        print(f'Running {notebook_path} successful')\n",
    "        \n",
    "        # save graded notebook\n",
    "        converted_notebook_path = notebook_path.replace('.ipynb', '-graded.ipynb')\n",
    "        with open(converted_notebook_path, mode='w', encoding='utf-8') as f:\n",
    "            nbformat.write(nb, f)\n",
    "        \n",
    "        # rename graded result JSON file\n",
    "        graded_result_json_path = notebook_path.replace('.ipynb', '-result.json')\n",
    "        shutil.move('lambdagrader-result.json', graded_result_json_path)\n",
    "        print(f'Stored graded result as JSON to {graded_result_json_path}')\n",
    "        \n",
    "        # read graded result to generate a summary\n",
    "        with open(graded_result_json_path, mode='r') as f:\n",
    "            graded_result = json.load(f)\n",
    "            \n",
    "        graded_notebook_filename = Path(converted_notebook_path).name\n",
    "            \n",
    "        summary = ''\n",
    "        summary += f\"File: {graded_notebook_filename}\\n\"\n",
    "        summary += f\"Score: {graded_result['learner_score']} out of {graded_result['total_available']}\\n\"\n",
    "        summary += f\"Passed {graded_result['num_passed_cases']} out of {graded_result['num_test_cases']} test cases\\n\"\n",
    "        summary += f\"Grading took {graded_result['grading_duration_in_seconds']} seconds\\n\\n\"\n",
    "        summary += 'Test Case Summary\\n'\n",
    "        \n",
    "        for o in graded_result['results']:\n",
    "            summary += \"-----------------\\n\"\n",
    "            summary += f\"{o['test_case_name']} {'passed' if o['pass'] else 'failed'}: {o['points']} out of {o['available_points']} points\\n\"\n",
    "            \n",
    "            if not o['pass']:\n",
    "                summary += f\"[Autograder Output]\\n{o['message']}\\n\\n\"\n",
    "                \n",
    "        result_summary = {\n",
    "            'filename': graded_notebook_filename,\n",
    "            'grading_finished_at': graded_result['grading_finished_at'],\n",
    "            'grading_duration_in_seconds': graded_result['grading_duration_in_seconds'],\n",
    "            'learner_score': graded_result['learner_score'],\n",
    "            'total_available': graded_result['total_available'],\n",
    "            'num_test_cases': graded_result['num_test_cases'],\n",
    "            'num_passed_cases': graded_result['num_passed_cases'],\n",
    "            'num_failed_cases': graded_result['num_failed_cases'],\n",
    "            'summary': summary\n",
    "        }\n",
    "\n",
    "        # remove prepend, append cells added by LambdaGrader before storing to HTML\n",
    "        nb.cells.pop(0)  # first cell (added by LambdaGrader)\n",
    "        nb.cells.pop()   # last cell (added by LambdaGrader)\n",
    "        \n",
    "        insert_index = 0\n",
    "        \n",
    "        # add result summary\n",
    "        nb.cells.insert(insert_index, new_markdown_cell('#  LambdaGrader Summary'))\n",
    "        insert_index += 1\n",
    "        \n",
    "        nb.cells.insert(insert_index, new_markdown_cell('## Metadata'))\n",
    "        insert_index += 1\n",
    "        \n",
    "        df_metadata = pd.DataFrame({\n",
    "            'name': [\n",
    "                'graded_filename',\n",
    "                'grading_finished_at',\n",
    "                'grading_duration',\n",
    "                '**learner_score**',\n",
    "                'max_score',\n",
    "                'learner_score_in_percentage',\n",
    "                'num_test_cases',\n",
    "                'num_passed_cases',\n",
    "                'num_failed_cases'\n",
    "            ],\n",
    "            'value': [\n",
    "                graded_notebook_filename,\n",
    "                graded_result['grading_finished_at'],\n",
    "                f\"{graded_result['grading_duration_in_seconds']} second{'' if graded_result['grading_duration_in_seconds'] == 0 else 's'}\",\n",
    "                f\"**{graded_result['learner_score']}**\",\n",
    "                graded_result['total_available'],\n",
    "                f\"{round(graded_result['learner_score'] / graded_result['total_available'] * 100, 2)}%\",\n",
    "                graded_result['num_test_cases'],\n",
    "                graded_result['num_passed_cases'],\n",
    "                graded_result['num_failed_cases']\n",
    "            ]\n",
    "        })\n",
    "        nb.cells.insert(insert_index, new_markdown_cell(df_metadata.to_markdown(index=False)))\n",
    "        insert_index += 1\n",
    "        \n",
    "        nb.cells.insert(insert_index, new_markdown_cell('## Test case results'))\n",
    "        insert_index += 1\n",
    "        \n",
    "        df_r = pd.DataFrame(graded_result['results'])\n",
    "        df_r['pass'] = df_r['pass'].map({\n",
    "            True: ' Pass', False: ' Fail'\n",
    "        })\n",
    "        # \n",
    "        df_r.rename(columns={\n",
    "            'available_points': 'max_score',\n",
    "            'points': 'learner_score',\n",
    "            'pass': 'result'\n",
    "        }, inplace=True)\n",
    "        \n",
    "        nb.cells.insert(insert_index, new_markdown_cell(df_r.to_markdown()))\n",
    "        insert_index += 1\n",
    "        \n",
    "        nb.cells.insert(insert_index, new_markdown_cell('\\n---\\n'))\n",
    "        insert_index += 1\n",
    "        \n",
    "        # store graded result to HTML\n",
    "        graded_html_path = notebook_path.replace('.ipynb', '-graded.html')\n",
    "        html_exporter = HTMLExporter()\n",
    "        r = html_exporter.from_notebook_node(nb)\n",
    "        with open(graded_html_path, 'w', encoding=\"utf-8\") as f:\n",
    "            f.write(r[0])\n",
    "        \n",
    "        print(result_summary)\n",
    "        graded_results.append(result_summary)\n",
    "    except CellExecutionError as e:\n",
    "        print(f'CellExecutionError on {notebook_path}')\n",
    "        print('-----------------------------')\n",
    "        print(e)\n",
    "        \n",
    "df_summary = pd.DataFrame(graded_results)\n",
    "\n",
    "df_summary.to_csv(\n",
    "    f\"graded_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\",\n",
    "    index=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambdagrader import *\n",
    "\n",
    "tcs = extract_test_cases_metadata_from_notebook( '../notebooks\\\\case-study-04-rideshare-trips-SOLUTION.ipynb')\n",
    "\n",
    "s = 0\n",
    "\n",
    "for o in tcs:\n",
    "    s += o['points']\n",
    "    \n",
    "print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
